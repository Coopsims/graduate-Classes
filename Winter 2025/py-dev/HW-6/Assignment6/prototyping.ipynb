{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Week 6 Assignment - Pandas\n"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T16:17:02.067871Z",
     "start_time": "2025-02-16T16:17:01.917733Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Load the data into a pandas dataframe (you may get a warning, you can get rid of it by setting low_memory=False). \n",
    "\n",
    "### Print the first 10 rows and print a random sampling of the rows in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T16:17:02.748867Z",
     "start_time": "2025-02-16T16:17:02.071766Z"
    }
   },
   "source": [
    "df = pd.read_csv(\"data/realtor-data.csv\", low_memory=False)\n",
    "\n",
    "print(\"First 10 rows:\")\n",
    "print(df.head(10))\n",
    "\n",
    "print(\"\\nRandom sample of 5 rows:\")\n",
    "print(df.sample(5))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows:\n",
      "     status  bed bath  acre_lot           city        state  zip_code  \\\n",
      "0  for_sale  3.0  2.0      0.12       Adjuntas  Puerto Rico     601.0   \n",
      "1  for_sale  4.0  2.0      0.08       Adjuntas  Puerto Rico     601.0   \n",
      "2  for_sale  2.0  1.0      0.15     Juana Diaz  Puerto Rico     795.0   \n",
      "3  for_sale  4.0  2.0      0.10          Ponce  Puerto Rico     731.0   \n",
      "4  for_sale  6.0  2.0      0.05       Mayaguez  Puerto Rico     680.0   \n",
      "5  for_sale  4.0  3.0      0.46  San Sebastian  Puerto Rico     612.0   \n",
      "6  for_sale  3.0  1.0      0.20         Ciales  Puerto Rico     639.0   \n",
      "7  for_sale  3.0  2.0      0.08          Ponce  Puerto Rico     731.0   \n",
      "8  for_sale  2.0  1.0      0.09          Ponce  Puerto Rico     730.0   \n",
      "9  for_sale  5.0  3.0      7.46     Las Marias  Puerto Rico     670.0   \n",
      "\n",
      "   house_size prev_sold_date     price  \n",
      "0       920.0            NaN  105000.0  \n",
      "1      1527.0            NaN   80000.0  \n",
      "2       748.0            NaN   67000.0  \n",
      "3      1800.0            NaN  145000.0  \n",
      "4         NaN            NaN   65000.0  \n",
      "5      2520.0            NaN  179000.0  \n",
      "6      2040.0            NaN   50000.0  \n",
      "7      1050.0            NaN   71600.0  \n",
      "8      1092.0            NaN  100000.0  \n",
      "9      5403.0            NaN  300000.0  \n",
      "\n",
      "Random sample of 5 rows:\n",
      "           status  bed bath  acre_lot        city        state  zip_code  \\\n",
      "356427   for_sale  NaN  NaN     26.28  Colchester      Vermont    5446.0   \n",
      "20379    for_sale  3.0  1.0      0.06     Bayamon  Puerto Rico     956.0   \n",
      "753177   for_sale  2.0  2.0       NaN    New York     New York   11231.0   \n",
      "1000101  for_sale  4.0  3.0      0.46      Monroe     New York   10950.0   \n",
      "665157   for_sale  NaN  NaN       NaN  Manahawkin   New Jersey    8050.0   \n",
      "\n",
      "         house_size prev_sold_date      price  \n",
      "356427          NaN            NaN  3500000.0  \n",
      "20379         896.0            NaN   105000.0  \n",
      "753177       1092.0            NaN  1250000.0  \n",
      "1000101      2270.0     2002-07-18   619000.0  \n",
      "665157          NaN     2022-02-25   139000.0  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) You should always check how many null values there are in your data as well as the data types of the data you're working with. Often you will come across data that looks correct but isn't the right data type. \n",
    "\n",
    "### Check the number of null values for every column and check the data types as well"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T16:17:02.991700Z",
     "start_time": "2025-02-16T16:17:02.816342Z"
    }
   },
   "source": [
    "print(\"Null values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nData types of columns:\")\n",
    "print(df.dtypes)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values per column:\n",
      "status                 0\n",
      "bed               216467\n",
      "bath              194206\n",
      "acre_lot          357467\n",
      "city                 191\n",
      "state                  0\n",
      "zip_code             479\n",
      "house_size        450112\n",
      "prev_sold_date    686293\n",
      "price                108\n",
      "dtype: int64\n",
      "\n",
      "Data types of columns:\n",
      "status             object\n",
      "bed                object\n",
      "bath               object\n",
      "acre_lot          float64\n",
      "city               object\n",
      "state              object\n",
      "zip_code          float64\n",
      "house_size        float64\n",
      "prev_sold_date     object\n",
      "price              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) We have 3 columns that looked right when checking the data but aren't the right data type and we'll correct it. \n",
    "\n",
    "### Cast the columns bed, bath and price to float. Values that cannot be casted to float, like \"hello\" should be turned into NaN. \n",
    "\n",
    "### Check the data types again to make sure the conversion was successfull.\n",
    "\n",
    "\n",
    "\n",
    "### Get a count of the number of NaNs in bed, bath and price columns. \n",
    "\n",
    "### You should get 216535, 194215 and 110 respectively\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T16:17:03.582005Z",
     "start_time": "2025-02-16T16:17:02.996806Z"
    }
   },
   "source": [
    "for col in ['bed', 'bath', 'price']:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "print(\"\\nData types after conversion:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\nNull counts after conversion:\")\n",
    "print(\"bed:\", df['bed'].isnull().sum())\n",
    "print(\"bath:\", df['bath'].isnull().sum())\n",
    "print(\"price:\", df['price'].isnull().sum())\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data types after conversion:\n",
      "status             object\n",
      "bed               float64\n",
      "bath              float64\n",
      "acre_lot          float64\n",
      "city               object\n",
      "state              object\n",
      "zip_code          float64\n",
      "house_size        float64\n",
      "prev_sold_date     object\n",
      "price             float64\n",
      "dtype: object\n",
      "\n",
      "Null counts after conversion:\n",
      "bed: 216535\n",
      "bath: 194215\n",
      "price: 110\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Check the number of unique values in the bed, bath and state columns. \n",
    "\n",
    "### You should get 49, 42 and 19 respectively\n",
    "\n",
    "### Print the uniques values for bed, bath and state. What do you notice about the unique values ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T16:17:57.854474Z",
     "start_time": "2025-02-16T16:17:57.754030Z"
    }
   },
   "source": [
    "print(\"\\nUnique values count:\")\n",
    "print(\"bed:\", df['bed'].nunique())\n",
    "print(\"bath:\", df['bath'].nunique())\n",
    "print(\"state:\", df['state'].nunique())\n",
    "\n",
    "print(\"\\nUnique values:\")\n",
    "print(\"bed:\", df['bed'].unique())\n",
    "print(\"bath:\", df['bath'].unique())\n",
    "print(\"state:\", df['state'].unique())\n",
    "\n",
    "\"\"\"\n",
    "there are some weird vals like 68. 123, inf\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique values count:\n",
      "bed: 49\n",
      "bath: 42\n",
      "state: 19\n",
      "\n",
      "Unique values:\n",
      "bed: [  3.   4.   2.   6.   5.   1.   9.  nan   7.   8.  12.  13.  10.  11.\n",
      "  33.  24.  28.  14.  18.  20.  16.  15.  19.  17.  40.  21.  86.  31.\n",
      "  27.  42.  60.  22.  32.  99.  49.  29.  30.  23.  46.  36.  68. 123.\n",
      "  25.  47.  inf  35.  38.  64.  48.  75.]\n",
      "bath: [  2.   1.   3.   5.   4.   7.   6.  nan   8.   9.  10.  12.  13.  35.\n",
      "  11.  16.  15.  18.  20.  14.  36.  25.  17.  19.  56.  42.  51.  28.\n",
      " 198.  22.  33.  27.  30.  29.  24.  46.  21. 123.  39.  43.  32.  45.\n",
      "  64.]\n",
      "state: ['Puerto Rico' 'Virgin Islands' 'Massachusetts' 'Connecticut'\n",
      " 'New Hampshire' 'Vermont' 'New Jersey' 'New York' 'South Carolina'\n",
      " 'Tennessee' 'Rhode Island' 'Virginia' 'Wyoming' 'Maine' 'Georgia'\n",
      " 'Pennsylvania' 'West Virginia' 'Delaware' 'Louisiana']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) We want to see which state has the largest number of properties for sale. \n",
    "\n",
    "### Print a count of the number of properties in each state/territory. \n",
    "\n",
    "### We want to make sure that we're getting unique listings, so drop any duplicate rows and print the count of the number of properties. What do you notice about the number of properties in each state ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T16:17:03.951779Z",
     "start_time": "2025-02-16T16:17:03.684857Z"
    }
   },
   "source": [
    "print(\"\\nProperties count by state:\")\n",
    "print(df['state'].value_counts())\n",
    "\n",
    "df_unique = df.drop_duplicates()\n",
    "print(\"\\nProperties count by state after dropping duplicates:\")\n",
    "print(df_unique['state'].value_counts())\n",
    "\"\"\"\n",
    "THere are many duplicate listings in the data, also most of the data is from the new england area\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Properties count by state:\n",
      "state\n",
      "New York          653061\n",
      "New Jersey        256551\n",
      "Massachusetts     177170\n",
      "Connecticut        98816\n",
      "New Hampshire      51394\n",
      "Vermont            48230\n",
      "Maine              36650\n",
      "Rhode Island       29610\n",
      "Puerto Rico        24679\n",
      "Pennsylvania       20060\n",
      "Virgin Islands      2573\n",
      "Delaware            2135\n",
      "Georgia               50\n",
      "Virginia              31\n",
      "South Carolina        25\n",
      "Tennessee             20\n",
      "West Virginia          5\n",
      "Wyoming                3\n",
      "Louisiana              3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Properties count by state after dropping duplicates:\n",
      "state\n",
      "New York          67160\n",
      "New Jersey        32601\n",
      "Connecticut       13753\n",
      "Massachusetts     10056\n",
      "Pennsylvania       9549\n",
      "Maine              4938\n",
      "New Hampshire      3431\n",
      "Rhode Island       3332\n",
      "Puerto Rico        2651\n",
      "Vermont            2544\n",
      "Delaware           1290\n",
      "Virgin Islands      730\n",
      "Virginia              7\n",
      "Georgia               5\n",
      "West Virginia         1\n",
      "Tennessee             1\n",
      "Wyoming               1\n",
      "South Carolina        1\n",
      "Louisiana             1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTHere are many duplicate listings in the data\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) We now want to look for patterns in our data, find the 5 dates when the most houses were sold. What do you notice ?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T16:17:03.991156Z",
     "start_time": "2025-02-16T16:17:03.958581Z"
    }
   },
   "source": [
    "print(\"\\nTop 5 dates (prev_sold_date) with most sales:\")\n",
    "print(df['prev_sold_date'].value_counts().head(5))\n",
    "\n",
    "\"\"\"\n",
    "top 5 dates are all in 2022 and during the spring months. this fits with the trend from other sources I work with that spring is when the housing market is most active and covid inflated the number of houses sold.\n",
    "\"\"\""
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 dates (prev_sold_date) with most sales:\n",
      "prev_sold_date\n",
      "2022-04-15    734\n",
      "2022-02-28    554\n",
      "2022-03-31    516\n",
      "2021-10-13    478\n",
      "2022-01-21    433\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntop 5 dates are all in 2022 and during the spring months. this fits with the trend from other sources I work with that spring is when the housing market is most active and covid inflated the number of houses sold.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Now we want to create a simple but effective summary of the properties that are for sale. \n",
    "\n",
    "### Let's create a summary table that contains the average home size and price, every state and each city within a state. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-16T16:17:04.095467Z",
     "start_time": "2025-02-16T16:17:03.998472Z"
    }
   },
   "source": [
    "summary_table = df.groupby(['state', 'city']).agg({'house_size': 'mean', 'price': 'mean'})\n",
    "print(\"\\nSummary table (average house_size and price by state and city):\")\n",
    "print(summary_table)\n",
    "#Your output should be this:\n",
    "# \t\t                          house_size\tprice\n",
    "# state\t            city\t\t\n",
    "# Connecticut\t    Andover\t     1653.750000\t2.539500e+05\n",
    "#                   Ansonia\t     1848.172414\t2.917902e+05\n",
    "#                   Ashford\t     1638.888889\t1.959045e+05\n",
    "#                   Avon\t     2929.878788\t5.824611e+05\n",
    "#                   Barkhamsted\t 2703.538462\t3.383238e+05\n",
    "# ...\t...\t...\t...\n",
    "# Virgin Islands\tSaint Thomas 3435.025641\t1.185128e+06\n",
    "# Virginia\t        Cape Charles\t     NaN\t7.130000e+05\n",
    "#                   Chincoteague\t     NaN\t1.620000e+05\n",
    "# West Virginia\t    Wyoming\t     1860.000000\t6.250000e+04\n",
    "# Wyoming\t        Cody\t     1935.000000\t5.350000e+05\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary table (average house_size and price by state and city):\n",
      "                              house_size         price\n",
      "state          city                                   \n",
      "Connecticut    Andover       1607.180328  2.623527e+05\n",
      "               Ansonia       1840.066372  2.939403e+05\n",
      "               Ashford       1648.345324  2.762310e+05\n",
      "               Avon          2977.006965  6.036860e+05\n",
      "               Barkhamsted   2411.147783  3.866785e+05\n",
      "...                                  ...           ...\n",
      "Virgin Islands Saint Thomas  3483.603448  1.169000e+06\n",
      "Virginia       Cape Charles          NaN  7.100000e+05\n",
      "               Chincoteague          NaN  1.707000e+05\n",
      "West Virginia  Wyoming       1860.000000  6.250000e+04\n",
      "Wyoming        Cody          1935.000000  5.350000e+05\n",
      "\n",
      "[4308 rows x 2 columns]\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
